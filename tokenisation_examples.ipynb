{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisation Examples using NLTK (Natural Language Processing Toolkit)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\thnga\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\thnga\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\thnga\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\thnga\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\thnga\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thnga\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\thnga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenisation into individual words, ['Hello', ',', 'I', 'am', 'a', 'coder', '.', 'I', 'like', 'to', 'code', 'various', 'applications', 'and', 'programs', 'using', 'my', 'skill', '.', 'Some', 'people', 'view', 'coding', 'as', 'a', 'chore', ',', 'but', 'to', 'me', 'it', \"'s\", 'a', 'priviledge', 'endowed', 'to', 'me', 'by', 'Mother', 'Nature', 'by', 'virtue', 'of', 'my', 'passion', 'for', 'computers', '!', '!', '!']\n",
      "Tokenisation into sentences, ['\\nHello, I am a coder.', 'I like to code various applications and programs using my skill.', \"Some people view coding as a chore, but to me it's a priviledge endowed to me by Mother Nature by \\nvirtue of my passion for computers!!\", '!']\n",
      "Tokenisation into words keeping tabs of punctuation as well, ['Hello', ',', 'I', 'am', 'a', 'coder', '.', 'I', 'like', 'to', 'code', 'various', 'applications', 'and', 'programs', 'using', 'my', 'skill', '.', 'Some', 'people', 'view', 'coding', 'as', 'a', 'chore', ',', 'but', 'to', 'me', 'it', \"'\", 's', 'a', 'priviledge', 'endowed', 'to', 'me', 'by', 'Mother', 'Nature', 'by', 'virtue', 'of', 'my', 'passion', 'for', 'computers', '!!!']\n",
      "Vocabulary List: ['Hello', ',', 'I', 'am', 'a', 'coder', '.', 'like', 'to', 'code', 'various', 'applications', 'and', 'programs', 'using', 'my', 'skill', 'Some', 'people', 'view', 'coding', 'as', 'chore', 'but', 'me', 'it', \"'s\", 'priviledge', 'endowed', 'by', 'Mother', 'Nature', 'virtue', 'of', 'passion', 'for', 'computers', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "\n",
    "# A corpus is a whole paragraph of text\n",
    "# A document is a sentence of text\n",
    "# A word is a single word\n",
    "# Vocabulary are all of the unique words in a corpus / document\n",
    "\n",
    "corpus = '''\n",
    "Hello, I am a coder. I like to code various applications and programs using my skill. \n",
    "Some people view coding as a chore, but to me it's a priviledge endowed to me by Mother Nature by \n",
    "virtue of my passion for computers!!!\n",
    "'''\n",
    "\n",
    "tokenizeByWords = word_tokenize(corpus)\n",
    "tokenizeBySentence = sent_tokenize(corpus)\n",
    "tokenizeByWordsPunct = wordpunct_tokenize(corpus)\n",
    "\n",
    "print(\"Tokenisation into individual words, {0}\".format(tokenizeByWords))\n",
    "print(\"Tokenisation into sentences, {0}\".format(tokenizeBySentence))\n",
    "print(\"Tokenisation into words keeping tabs of punctuation as well, {0}\".format(tokenizeByWordsPunct))\n",
    "\n",
    "#Generate Vocabulary list\n",
    "def findUnique(tokenisedCorpus):\n",
    "    seen = []\n",
    "    for word in tokenisedCorpus:\n",
    "        if word in seen:\n",
    "            continue\n",
    "        else:\n",
    "            seen.append(word)\n",
    "\n",
    "    return seen\n",
    "\n",
    "\n",
    "#Note: this can only be applied to a corpus that has been tokenised into its individual words and punctuations! Sentence structures themselves need to be broken down further...\n",
    "print(\"Vocabulary List: {0}\".format(findUnique(tokenisedCorpus=tokenizeByWords)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
